{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca176ac",
   "metadata": {},
   "source": [
    "#### Zadanie:\n",
    "Pomocou knižnice nltk v Pythone extrahujte lemy slov (základy slov) z anglických textov získaných z webu (Texty z webu.zip).\n",
    "\n",
    "Následne vytvorte dátovú maticu, ktorá bude obsahovať frekvenciu lém v jednotlivých dokumentoch, z ktorej odvodíte binárnu, logaritmickú a inverznú frekvenciu.\n",
    "\n",
    "Odovzdajte výsledný súbor a zdrojový kód v jednom zip súbore.\n",
    "\n",
    "Importy v Pythone vhodné pre riešenie tohto zadania: \n",
    "\n",
    "`import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))`\n",
    "\n",
    "Postup dosiahnutia výsledkov:\n",
    "\n",
    "načítajte texty do Pythonu;\n",
    "rozdeľte vety na slová pomocou príkazu: wordsList = nltk.word_tokenize(word)\n",
    "inicializovať lematizátor: lemmatizer = WordNetLemmatizer();\n",
    "získajte základ slova (lemmu) pre dané slová: lemmatizer.lemmatize(word);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a749277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6aee0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the text files and load the content from them \n",
    "text1 = open(\"text1.txt\", \"r\").read()\n",
    "text2 = open(\"text2.txt\", \"r\").read()\n",
    "text3 = open(\"text3.txt\", \"r\").read()\n",
    "text4 = open(\"text4.txt\", \"r\").read()\n",
    "text5 = open(\"text5.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e74f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Since we analyze just the existance of the words, \n",
    "#    we need to put them to lower case first\n",
    "text1 = text1.lower()\n",
    "text2 = text2.lower()\n",
    "text3 = text3.lower()\n",
    "text4 = text4.lower()\n",
    "text5 = text5.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d31858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenize the text\n",
    "tokenized_text1=word_tokenize(text1)\n",
    "tokenized_text2=word_tokenize(text2)\n",
    "tokenized_text3=word_tokenize(text3)\n",
    "tokenized_text4=word_tokenize(text4)\n",
    "tokenized_text5=word_tokenize(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2350c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. get rid of the stop words\n",
    "no_stopwords_text1 = [w for w in tokenized_text1 if not w in stop_words]\n",
    "no_stopwords_text2 = [q for q in tokenized_text2 if not q in stop_words]\n",
    "no_stopwords_text3 = [z for z in tokenized_text3 if not z in stop_words]\n",
    "no_stopwords_text4 = [a for a in tokenized_text4 if not a in stop_words]\n",
    "no_stopwords_text5 = [b for b in tokenized_text5 if not b in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82552fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the word lematizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4bf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. lematize the text wich does not contain the stop words anymore\n",
    "lematized_text1=[lemmatizer.lemmatize(w) for w in no_stopwords_text1]\n",
    "lematized_text2=[lemmatizer.lemmatize(q) for q in no_stopwords_text2]\n",
    "lematized_text3=[lemmatizer.lemmatize(a) for a in no_stopwords_text3]\n",
    "lematized_text4=[lemmatizer.lemmatize(z) for z in no_stopwords_text4]\n",
    "lematized_text5=[lemmatizer.lemmatize(x) for x in no_stopwords_text5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b936f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list containing all the lematized words\n",
    "all_words = []\n",
    "all_words.extend(lematized_text1)\n",
    "all_words.extend(lematized_text2)\n",
    "all_words.extend(lematized_text3)\n",
    "all_words.extend(lematized_text4)\n",
    "all_words.extend(lematized_text5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffaf2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since some words are bound to be repeated in multiple texts\n",
    "# we want to get rid of it, theretore, we create a set of all unique words\n",
    "all_words = set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9542ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is done so the list can be alphabeticaly sorted\n",
    "# set by default does not allows this possibility\n",
    "# this step is pure cosmetics\n",
    "all_words = list(all_words)\n",
    "\n",
    "all_words.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1d3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the empty lists to store frequency of the occurance of the word\n",
    "text1_word_freq  = []\n",
    "text2_word_freq  = []\n",
    "text3_word_freq  = []\n",
    "text4_word_freq  = []\n",
    "text5_word_freq  = []\n",
    "\n",
    "for word in all_words:\n",
    "    text1_word_freq.append(lematized_text1.count(word))\n",
    "    text2_word_freq.append(lematized_text2.count(word))\n",
    "    text3_word_freq.append(lematized_text3.count(word))\n",
    "    text4_word_freq.append(lematized_text4.count(word))\n",
    "    text5_word_freq.append(lematized_text5.count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d331a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the list of thelist to be fed to the dataframe as a input data\n",
    "# this way each list will be a row in the dataframe\n",
    "# if we used the dictionary, we would need be creating columns\n",
    "words_data = [\n",
    "    text1_word_freq,\n",
    "    text2_word_freq,\n",
    "    text3_word_freq,\n",
    "    text4_word_freq,\n",
    "    text5_word_freq\n",
    "]\n",
    "frekvencia_slov = pd.DataFrame(words_data, columns = all_words, index = ['text1','text2','text3','text4','text5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbf6f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the math for the logartimic calculations\n",
    "# we can do two counts at the same time, both binary and logaritmical \n",
    "# in one iteration\n",
    "import math\n",
    "\n",
    "# creating empty lists to hold valued for the binary values for lemas in text\n",
    "binary_text1 = []\n",
    "binary_text2 = []\n",
    "binary_text3 = []\n",
    "binary_text4 = []\n",
    "binary_text5 = []\n",
    "\n",
    "# creating empty lists to hold valued for the log values for lemas in text\n",
    "log_text1 = []\n",
    "log_text2 = []\n",
    "log_text3 = []\n",
    "log_text4 = []\n",
    "log_text5 = []\n",
    "\n",
    "\n",
    "# for binary if the word occurs in text value is one, if not value is zero\n",
    "# The logarithm function ensures the \"attenuation\" \n",
    "# of the frequency of occurrence of words, i. scattering stabilization\n",
    "\n",
    "for freq in text1_word_freq:\n",
    "    if freq > 0 : \n",
    "        binary_text1.append(1)\n",
    "        log_text1.append(math.log10(freq))\n",
    "    else :\n",
    "        binary_text1.append(0)\n",
    "        log_text1.append(0)\n",
    "        \n",
    "for freq in text2_word_freq:\n",
    "    if freq > 0 : \n",
    "        binary_text2.append(1)\n",
    "        log_text2.append(math.log10(freq))\n",
    "    else :\n",
    "        binary_text2.append(0)\n",
    "        log_text2.append(0)\n",
    "        \n",
    "for freq in text3_word_freq:\n",
    "    if freq > 0  : \n",
    "        binary_text3.append(1)\n",
    "        log_text3.append(math.log10(freq))\n",
    "    else :\n",
    "        binary_text3.append(0)\n",
    "        log_text3.append(0)\n",
    "        \n",
    "for freq in text4_word_freq:\n",
    "    if freq > 0 : \n",
    "        binary_text4.append(1)\n",
    "        log_text4.append(math.log10(freq))\n",
    "    else :\n",
    "        binary_text4.append(0)\n",
    "        log_text4.append(0)\n",
    "        \n",
    "for freq in text5_word_freq:\n",
    "    if freq > 0 : \n",
    "        binary_text5.append(1)\n",
    "        log_text5.append(math.log10(freq))\n",
    "    else :\n",
    "        binary_text5.append(0)\n",
    "        log_text5.append(0)\n",
    "        \n",
    "sum_binary = []\n",
    "for i in range(len(all_words)):\n",
    "    sum_binary.append(binary_text1[i]+binary_text2[i]+binary_text3[i]+binary_text4[i]+binary_text5[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12bd3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = [\n",
    "    binary_text1,\n",
    "    binary_text2,\n",
    "    binary_text3,\n",
    "    binary_text4,\n",
    "    binary_text5,\n",
    "    sum_binary\n",
    "    ]\n",
    "binarna_frekvencia = pd.DataFrame(binary_data , columns = all_words, index = ['text1','text2','text3','text4','text5', 'sum'])\n",
    "\n",
    "log_data =[\n",
    "        log_text1,\n",
    "        log_text2,\n",
    "        log_text3,\n",
    "        log_text4,\n",
    "        log_text5,\n",
    "]\n",
    "\n",
    "log_frekvencia = pd.DataFrame(log_data, columns = all_words, index = ['text1','text2','text3','text4','text5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b650cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists to hold valued for the IDF (inverse document freq)\n",
    "# values for lemas in text\n",
    "binary_text1 = []\n",
    "inverse_text1 = []\n",
    "inverse_text2 = []\n",
    "inverse_text3 = []\n",
    "inverse_text4 = []\n",
    "inverse_text5 = []\n",
    "\n",
    "index = 0\n",
    "for i in text1_word_freq:\n",
    "    if i > 0 : \n",
    "        inverse_text1.append(log_text1[index] * math.log10(5/sum_binary[index]))\n",
    "    else :        \n",
    "        inverse_text1.append(0)\n",
    "    index += 1\n",
    "    \n",
    "index = 0\n",
    "for i in text2_word_freq:\n",
    "    if i > 0 : \n",
    "        inverse_text2.append(log_text2[index] * math.log10(5/sum_binary[index]))\n",
    "    else :        \n",
    "        inverse_text2.append(0)\n",
    "    index +=1\n",
    "    \n",
    "index = 0\n",
    "for i in text3_word_freq:\n",
    "    if i > 0 : \n",
    "        inverse_text3.append(log_text3[index] * math.log10(5/sum_binary[index]))\n",
    "    else :        \n",
    "        inverse_text3.append(0)\n",
    "    index +=1\n",
    "\n",
    "index = 0\n",
    "for i in text4_word_freq:\n",
    "    if i > 0 : \n",
    "        inverse_text4.append(log_text4[index] * math.log10(5/sum_binary[index]))\n",
    "    else :        \n",
    "        inverse_text4.append(0)\n",
    "    index += 1\n",
    "\n",
    "index = 0\n",
    "for i in text5_word_freq:\n",
    "    if i > 0 : \n",
    "        inverse_text5.append(log_text5[index] * math.log10(5/sum_binary[index]))\n",
    "    else :        \n",
    "        inverse_text5.append(0)\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac224037",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_data =[\n",
    "        inverse_text1,\n",
    "        inverse_text2,\n",
    "        inverse_text3,\n",
    "        inverse_text4,\n",
    "        inverse_text5,\n",
    "]\n",
    "\n",
    "inverse_frekvencia = pd.DataFrame(inverse_data, columns = all_words, index = ['text1','text2','text3','text4','text5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74b42f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writting to excel with multiple sheets\n",
    "with pd.ExcelWriter('Reprezentacia_textu_II.xlsx') as writer:\n",
    "    frekvencia_slov.to_excel(writer, sheet_name='frekvencia_slov')\n",
    "    binarna_frekvencia.to_excel(writer, sheet_name='binarna_frekvencia')\n",
    "    log_frekvencia.to_excel(writer, sheet_name='logartimicka_frekvencia')\n",
    "    inverse_frekvencia.to_excel(writer, sheet_name='inversna_frekvencia')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
